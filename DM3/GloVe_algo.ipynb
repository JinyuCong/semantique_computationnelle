{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GloVe Algorithme",
   "id": "e0539fdb9eb3c8a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "corpus = [\n",
    "    \"the king is a man who rules a kingdom\",\n",
    "    \"the queen is a woman who rules a kingdom\",\n",
    "    \"the man is strong and wise\",\n",
    "    \"the woman is graceful and intelligent\",\n",
    "    \"the king married the queen to unite their kingdoms\",\n",
    "    \"a queen can reign in the absence of a king\",\n",
    "    \"the man aspired to be a king one day\",\n",
    "    \"the woman aspired to be a queen one day\",\n",
    "    \"king and queen often host grand ceremonies\",\n",
    "    \"the king and the queen govern the kingdom together\",\n",
    "]\n",
    "\n",
    "def build_vocab(corpus):\n",
    "    vocab = set([word for sentence in corpus for word in sentence.split()])\n",
    "    word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
    "    id_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "    return vocab, word_to_id, id_to_word\n",
    "\n",
    "vocab, word_to_id, id_to_word = build_vocab(corpus)\n",
    "vocab_size = len(vocab)\n",
    "print(word_to_id)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_cooccurrence_matrix(corpus, word_to_id, window_size=5):\n",
    "    coocurrence_matrix = np.zeros((vocab_size, vocab_size))\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for idx, word in enumerate(words):\n",
    "            word_id = word_to_id[word]\n",
    "            neighbours = words[max(idx-window_size, 0) : min(idx+window_size+1, len(words))]\n",
    "            for neighbour in neighbours:\n",
    "                if word != neighbour:\n",
    "                    neighbour_id = word_to_id[neighbour]\n",
    "                    coocurrence_matrix[word_id, neighbour_id] += 1\n",
    "    return coocurrence_matrix\n",
    "\n",
    "coocurrence_matrix = build_cooccurrence_matrix(corpus, word_to_id)\n",
    "coocurrence_matrix\n",
    "            "
   ],
   "id": "bcc1e7f762a18bb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "embedding_dim = 50\n",
    "np.random.seed(42)\n",
    "W = np.random.rand(vocab_size, embedding_dim)\n",
    "W_context = np.random.rand(vocab_size, embedding_dim)\n",
    "bias = np.random.rand(vocab_size)\n",
    "bias_context = np.random.rand(vocab_size)\n",
    "\n",
    "def glove_loss(X, W, W_context, bias, bias_context, x_max=100, alpha=0.75):\n",
    "    loss = 0\n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            if X[i, j] > 0:\n",
    "                weight = (X[i, j]/x_max) ** alpha if X[i, j] < x_max else 1\n",
    "                J = weight * (W[i] @ W_context[j] + bias[i] + bias_context[j] - np.log(X[i, j])) ** 2\n",
    "                loss += J\n",
    "    loss /= vocab_size ** 2\n",
    "    return loss\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "x_max = 10\n",
    "alpha = 0.75\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            if coocurrence_matrix[i, j] > 0:\n",
    "                weight = (coocurrence_matrix[i, j] / x_max) ** alpha if coocurrence_matrix[i, j] < x_max else 1\n",
    "                diff = W[i] @ W_context[j] + bias[i] + bias_context[j] - np.log(coocurrence_matrix[i, j])\n",
    "                \n",
    "                grad_W = weight * diff * W_context[j]\n",
    "                grad_W_context = weight * diff * W[i]\n",
    "                grad_bias = weight * diff\n",
    "                grad_bias_context = weight * diff\n",
    "                \n",
    "                W[i] -= learning_rate * grad_W\n",
    "                W_context[j] -= learning_rate * grad_W_context\n",
    "                bias[i] -= learning_rate * grad_bias\n",
    "                bias_context[j] -= learning_rate * grad_bias_context\n",
    "            \n",
    "    if epoch % 10 == 0:\n",
    "        loss = glove_loss(coocurrence_matrix, W, W_context, bias, bias_context)\n",
    "        print(f\"epoch {epoch}, loss: {loss}\")\n",
    "        \n",
    "word_embeddings = W + W_context\n"
   ],
   "id": "4266482d63ff4e1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def search_analogy(word1, word2, word3, word_embeddings, word_to_id, id_to_word):\n",
    "    idx_word1 = word_to_id[word1]\n",
    "    idx_word2 = word_to_id[word2]\n",
    "    idx_word3 = word_to_id[word3]\n",
    "    \n",
    "    analogy = word_embeddings[idx_word1] - word_embeddings[idx_word2] + word_embeddings[idx_word3]\n",
    "    \n",
    "    nearest_distance = np.inf\n",
    "    most_similar_word = None\n",
    "    \n",
    "    for word_index, word_vector in enumerate(word_embeddings):\n",
    "        distance = np.linalg.norm(word_vector - analogy)\n",
    "        print(f\"{id_to_word[word_index]}, {distance}\")\n",
    "        if distance < nearest_distance:\n",
    "            nearest_distance = distance\n",
    "            most_similar_word = id_to_word[word_index]\n",
    "    return most_similar_word\n",
    "    \n",
    "search_analogy('king', 'man', 'woman', word_embeddings, word_to_id, id_to_word)\n",
    "    \n",
    "    "
   ],
   "id": "13c78bed5462bc77"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
