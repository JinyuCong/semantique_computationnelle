{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GloVe Algorithme",
   "id": "e0539fdb9eb3c8a5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-07T17:30:47.511878Z",
     "start_time": "2024-12-07T17:30:42.246518Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "corpus = [\n",
    "    \"the king is a man who rules a kingdom\",\n",
    "    \"the queen is a woman who rules a kingdom\",\n",
    "    \"the man is strong and wise\",\n",
    "    \"the woman is graceful and intelligent\",\n",
    "    \"the king married the queen to unite their kingdoms\",\n",
    "    \"a queen can reign in the absence of a king\",\n",
    "    \"the man aspired to be a king one day\",\n",
    "    \"the woman aspired to be a queen one day\",\n",
    "    \"king and queen often host grand ceremonies\",\n",
    "    \"the king and the queen govern the kingdom together\",\n",
    "]\n",
    "\n",
    "def build_vocab(corpus):\n",
    "    vocab = set([word for sentence in corpus for word in sentence.split()])\n",
    "    word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
    "    id_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "    return vocab, word_to_id, id_to_word\n",
    "\n",
    "vocab, word_to_id, id_to_word = build_vocab(corpus)\n",
    "vocab_size = len(vocab)\n",
    "print(word_to_id)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ceremonies': 0, 'married': 1, 'rules': 2, 'day': 3, 'woman': 4, 'man': 5, 'reign': 6, 'govern': 7, 'grand': 8, 'of': 9, 'a': 10, 'who': 11, 'aspired': 12, 'in': 13, 'absence': 14, 'one': 15, 'their': 16, 'and': 17, 'the': 18, 'intelligent': 19, 'wise': 20, 'unite': 21, 'host': 22, 'together': 23, 'to': 24, 'king': 25, 'queen': 26, 'is': 27, 'strong': 28, 'often': 29, 'be': 30, 'can': 31, 'kingdoms': 32, 'kingdom': 33, 'graceful': 34}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T17:30:48.285074Z",
     "start_time": "2024-12-07T17:30:48.279260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_cooccurrence_matrix(corpus, word_to_id, window_size=5):\n",
    "    coocurrence_matrix = np.zeros((vocab_size, vocab_size))\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for idx, word in enumerate(words):\n",
    "            word_id = word_to_id[word]\n",
    "            neighbours = words[max(idx-window_size, 0) : min(idx+window_size+1, len(words))]\n",
    "            for neighbour in neighbours:\n",
    "                if word != neighbour:\n",
    "                    neighbour_id = word_to_id[neighbour]\n",
    "                    coocurrence_matrix[word_id, neighbour_id] += 1\n",
    "    return coocurrence_matrix\n",
    "\n",
    "coocurrence_matrix = build_cooccurrence_matrix(corpus, word_to_id)\n",
    "coocurrence_matrix\n",
    "            "
   ],
   "id": "bcc1e7f762a18bb3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 2., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 2., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T17:30:49.419273Z",
     "start_time": "2024-12-07T17:30:49.206607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_dim = 50\n",
    "np.random.seed(42)\n",
    "W = np.random.rand(vocab_size, embedding_dim)\n",
    "W_context = np.random.rand(vocab_size, embedding_dim)\n",
    "bias = np.random.rand(vocab_size)\n",
    "bias_context = np.random.rand(vocab_size)\n",
    "\n",
    "def glove_loss(X, W, W_context, bias, bias_context, x_max=100, alpha=0.75):\n",
    "    loss = 0\n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            if X[i, j] > 0:\n",
    "                weight = (X[i, j]/x_max) ** alpha if X[i, j] < x_max else 1\n",
    "                J = weight * (W[i] @ W_context[j] + bias[i] + bias_context[j] - np.log(X[i, j])) ** 2\n",
    "                loss += J\n",
    "    loss /= vocab_size ** 2\n",
    "    return loss\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "x_max = 10\n",
    "alpha = 0.75\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            if coocurrence_matrix[i, j] > 0:\n",
    "                weight = (coocurrence_matrix[i, j] / x_max) ** alpha if coocurrence_matrix[i, j] < x_max else 1\n",
    "                diff = W[i] @ W_context[j] + bias[i] + bias_context[j] - np.log(coocurrence_matrix[i, j])\n",
    "                \n",
    "                grad_W = weight * diff * W_context[j]\n",
    "                grad_W_context = weight * diff * W[i]\n",
    "                grad_bias = weight * diff\n",
    "                grad_bias_context = weight * diff\n",
    "                \n",
    "                W[i] -= learning_rate * grad_W\n",
    "                W_context[j] -= learning_rate * grad_W_context\n",
    "                bias[i] -= learning_rate * grad_bias\n",
    "                bias_context[j] -= learning_rate * grad_bias_context\n",
    "            \n",
    "    if epoch % 10 == 0:\n",
    "        loss = glove_loss(coocurrence_matrix, W, W_context, bias, bias_context)\n",
    "        print(f\"epoch {epoch}, loss: {loss}\")\n",
    "        \n",
    "word_embeddings = W + W_context\n"
   ],
   "id": "4266482d63ff4e1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.5037277975815238\n",
      "epoch 10, loss: 0.027474525614644987\n",
      "epoch 20, loss: 0.007520292766907942\n",
      "epoch 30, loss: 0.0029732131917531816\n",
      "epoch 40, loss: 0.0014963563424770473\n",
      "epoch 50, loss: 0.000885435287521995\n",
      "epoch 60, loss: 0.0005795710357287714\n",
      "epoch 70, loss: 0.0004033112365595446\n",
      "epoch 80, loss: 0.00029154663312451153\n",
      "epoch 90, loss: 0.0002161046815966535\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T17:30:49.904482Z",
     "start_time": "2024-12-07T17:30:49.894658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def search_analogy(word1, word2, word3, word_embeddings, word_to_id, id_to_word):\n",
    "    idx_word1 = word_to_id[word1]\n",
    "    idx_word2 = word_to_id[word2]\n",
    "    idx_word3 = word_to_id[word3]\n",
    "    \n",
    "    analogy = word_embeddings[idx_word1] - word_embeddings[idx_word2] + word_embeddings[idx_word3]\n",
    "    \n",
    "    nearest_distance = np.inf\n",
    "    most_similar_word = None\n",
    "    \n",
    "    for word_index, word_vector in enumerate(word_embeddings):\n",
    "        distance = np.linalg.norm(word_vector - analogy)\n",
    "        print(f\"{id_to_word[word_index]}, {distance}\")\n",
    "        if distance < nearest_distance:\n",
    "            nearest_distance = distance\n",
    "            most_similar_word = id_to_word[word_index]\n",
    "    return most_similar_word\n",
    "    \n",
    "search_analogy('king', 'man', 'woman', word_embeddings, word_to_id, id_to_word)\n",
    "    \n",
    "    "
   ],
   "id": "13c78bed5462bc77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ceremonies, 5.515369111864009\n",
      "married, 5.815243849299505\n",
      "rules, 5.44058810356872\n",
      "day, 5.853181441406126\n",
      "woman, 3.4291271570638173\n",
      "man, 6.040977575232007\n",
      "reign, 4.649872527785306\n",
      "govern, 5.9988481611436235\n",
      "grand, 5.185778695661402\n",
      "of, 5.451295701388337\n",
      "a, 4.142715274763036\n",
      "who, 5.4617038863462355\n",
      "aspired, 4.902933485044255\n",
      "in, 4.844102795130681\n",
      "absence, 5.777869724997661\n",
      "one, 5.997296528745032\n",
      "their, 4.953411516018894\n",
      "and, 4.8068420865102315\n",
      "the, 4.727379151122312\n",
      "intelligent, 5.7391957323782705\n",
      "wise, 5.723526002679257\n",
      "unite, 5.4502375460451775\n",
      "host, 5.755791368769518\n",
      "together, 6.017254638557424\n",
      "to, 4.6654507446651925\n",
      "king, 3.692987251408548\n",
      "queen, 4.80188066956214\n",
      "is, 4.987628113671329\n",
      "strong, 5.2767207411149215\n",
      "often, 5.297463272123986\n",
      "be, 5.140922589322133\n",
      "can, 5.8588476301907235\n",
      "kingdoms, 6.21177694665099\n",
      "kingdom, 5.299561657255372\n",
      "graceful, 5.849915750667705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'woman'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2ac00b4834cb4d13"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
